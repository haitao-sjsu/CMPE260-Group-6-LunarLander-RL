{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ricRwOZH7-RG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "SUCCESS_REWARD = 100\n",
        "FAIL_REWARD = -100\n",
        "\n",
        "def evaluate_policy(\n",
        "    env: gym.Env,\n",
        "    policy,\n",
        "    num_episodes: int = 100,\n",
        "    seed: int | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate a given policy on the LunarLander (discrete) environment.\n",
        "\n",
        "    The policy is assumed to be a deterministic function:\n",
        "        action = policy(obs)\n",
        "\n",
        "    This function does NOT render. It only collects metrics.\n",
        "\n",
        "    Metrics returned (main keys):\n",
        "        - num_episodes\n",
        "        - mean_return, std_return, min_return, max_return\n",
        "        - solved_rate              # fraction of episodes with total_return >= 200\n",
        "        - success_rate             # fraction of episodes that ended in a \"safe landing\"\n",
        "        - crash_rate               # fraction of episodes that ended in a \"crash\"\n",
        "        - timeout_rate             # fraction of episodes that ended by truncation\n",
        "        - other_terminal_rate      # terminated but neither clearly success nor crash\n",
        "        - mean_episode_length\n",
        "        - mean_main_engine_usage   # how many steps per episode main engine fired (action == 1)\n",
        "        - mean_side_engine_usage   # how many steps per episode side engines fired (action == 2 or 3)\n",
        "\n",
        "        # Landing smoothness (computed only over success episodes):\n",
        "        - num_success_episodes\n",
        "        - mean_final_abs_x_success\n",
        "        - mean_final_abs_vy_success\n",
        "        - mean_final_abs_angle_success\n",
        "\n",
        "    Success / crash classification:\n",
        "        - We classify episodes only when `terminated == True` (not truncated).\n",
        "        - For the final step of an episode, we look at the *instant* reward:\n",
        "              last_step_reward > 0  -> success\n",
        "              last_step_reward < 0  -> crash\n",
        "              last_step_reward == 0 -> other_terminal\n",
        "        - This is a heuristic based on the environment design:\n",
        "              +100 additional reward for a safe landing\n",
        "              -100 additional reward for a crash\n",
        "          which strongly dominates other shaping terms.\n",
        "    \"\"\"\n",
        "\n",
        "    # Store per-episode statistics\n",
        "    returns = []\n",
        "    steps_list = []\n",
        "    main_usage_list = []   # count of steps where main engine fired (action == 1)\n",
        "    side_usage_list = []   # count of steps where side engines fired (action == 2 or 3)\n",
        "\n",
        "    # Counters for episode termination types\n",
        "    num_success = 0        # safe landing (positive last-step reward on terminated)\n",
        "    num_crash = 0          # crash (negative last-step reward on terminated)\n",
        "    num_timeout = 0        # truncated episodes (time limit or similar)\n",
        "    num_other_end = 0      # terminated but last-step reward == 0 (rare / ambiguous)\n",
        "\n",
        "    # Landing smoothness metrics for success episodes only\n",
        "    success_final_x = []       # |x| at the end\n",
        "    success_final_vy = []      # |vy| at the end\n",
        "    success_final_angle = []   # |angle| at the end\n",
        "\n",
        "    # Initial reset: use seed only for the very first episode (if provided)\n",
        "    if seed is not None:\n",
        "        obs, info = env.reset(seed=seed)\n",
        "    else:\n",
        "        obs, info = env.reset()\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "        steps = 0\n",
        "        main_usage = 0\n",
        "        side_usage = 0\n",
        "        last_step_reward = 0.0   # will be overwritten at each step\n",
        "\n",
        "        while not done:\n",
        "            # Policy must be a function: action = policy(obs)\n",
        "            action = policy(obs)\n",
        "\n",
        "            # Gymnasium step API: obs, reward, terminated, truncated, info\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "            last_step_reward = reward\n",
        "\n",
        "            # Count engine usage based on discrete action:\n",
        "            # 0: do nothing\n",
        "            # 1: main engine\n",
        "            # 2: side engine (left)\n",
        "            # 3: side engine (right)\n",
        "            if action == 1:\n",
        "                main_usage += 1\n",
        "            elif action == 2 or action == 3:\n",
        "                side_usage += 1\n",
        "\n",
        "            done = terminated or truncated\n",
        "\n",
        "        # At this point, `obs` is the final state of the episode.\n",
        "        final_state = obs\n",
        "        # For LunarLander, state is typically:\n",
        "        # [x, y, vx, vy, angle, v_angle, left_leg_contact, right_leg_contact]\n",
        "        final_x = float(abs(final_state[0]))    # horizontal offset from center\n",
        "        final_vy = float(abs(final_state[3]))   # vertical speed magnitude\n",
        "        final_angle = float(abs(final_state[4]))  # absolute tilt angle\n",
        "\n",
        "        # Store basic episode statistics\n",
        "        returns.append(total_reward)\n",
        "        steps_list.append(steps)\n",
        "        main_usage_list.append(main_usage)\n",
        "        side_usage_list.append(side_usage)\n",
        "\n",
        "        # Classify how the episode ended\n",
        "        if truncated:\n",
        "            # Episode ended due to time limit or external truncation\n",
        "            num_timeout += 1\n",
        "        elif terminated:\n",
        "            # Episode ended naturally; decide between success / crash / other\n",
        "            if last_step_reward == SUCCESS_REWARD:\n",
        "                # Safe landing (heuristic: last step reward positive due to +100 bonus)\n",
        "                num_success += 1\n",
        "                # Record landing smoothness metrics\n",
        "                success_final_x.append(final_x)\n",
        "                success_final_vy.append(final_vy)\n",
        "                success_final_angle.append(final_angle)\n",
        "            elif last_step_reward == FAIL_REWARD:\n",
        "                # Crash (heuristic: last step reward negative due to -100 penalty)\n",
        "                num_crash += 1\n",
        "            else:\n",
        "                # Rare / ambiguous case: terminated but last-step reward exactly 0\n",
        "                num_other_end += 1\n",
        "\n",
        "        # Reset for the next episode (without re-seeding)\n",
        "        obs, info = env.reset()\n",
        "\n",
        "    # Convert lists to numpy arrays for easier statistics\n",
        "    returns = np.array(returns, dtype=np.float32)\n",
        "    steps_arr = np.array(steps_list, dtype=np.int32)\n",
        "    main_usage_arr = np.array(main_usage_list, dtype=np.int32)\n",
        "    side_usage_arr = np.array(side_usage_list, dtype=np.int32)\n",
        "\n",
        "    # Basic return statistics\n",
        "    mean_return = float(returns.mean())\n",
        "    std_return = float(returns.std())\n",
        "    min_return = float(returns.min())\n",
        "    max_return = float(returns.max())\n",
        "\n",
        "    # Fraction of episodes with total return >= 200 (standard \"solved\" threshold)\n",
        "    solved_rate = float(np.mean(returns >= 200.0))\n",
        "\n",
        "    # Termination-type rates\n",
        "    n = num_episodes\n",
        "    success_rate = num_success / n\n",
        "    crash_rate = num_crash / n\n",
        "    timeout_rate = num_timeout / n\n",
        "    other_terminal_rate = num_other_end / n\n",
        "\n",
        "    # Episode length and engine usage\n",
        "    mean_episode_length = float(steps_arr.mean())\n",
        "    mean_main_engine_usage = float(main_usage_arr.mean())\n",
        "    mean_side_engine_usage = float(side_usage_arr.mean())\n",
        "\n",
        "    metrics = {\n",
        "        \"num_episodes\": n,\n",
        "        # Return statistics\n",
        "        \"mean_return\": mean_return,\n",
        "        \"std_return\": std_return,\n",
        "        \"min_return\": min_return,\n",
        "        \"max_return\": max_return,\n",
        "        \"solved_rate\": solved_rate,\n",
        "        # Termination statistics\n",
        "        \"success_rate\": success_rate,\n",
        "        \"crash_rate\": crash_rate,\n",
        "        \"timeout_rate\": timeout_rate,\n",
        "        \"other_terminal_rate\": other_terminal_rate,\n",
        "        # Episode length & engine usage\n",
        "        \"mean_episode_length\": mean_episode_length,\n",
        "        \"mean_main_engine_usage\": mean_main_engine_usage,\n",
        "        \"mean_side_engine_usage\": mean_side_engine_usage,\n",
        "        # Placeholder, will be filled below if there are success episodes\n",
        "        \"num_success_episodes\": num_success,\n",
        "    }\n",
        "\n",
        "    # Landing smoothness metrics over success episodes (if any)\n",
        "    if num_success > 0:\n",
        "        success_final_x = np.array(success_final_x, dtype=np.float32)\n",
        "        success_final_vy = np.array(success_final_vy, dtype=np.float32)\n",
        "        success_final_angle = np.array(success_final_angle, dtype=np.float32)\n",
        "\n",
        "        metrics.update(\n",
        "            {\n",
        "                \"mean_final_abs_x_success\": float(success_final_x.mean()),\n",
        "                \"mean_final_abs_vy_success\": float(success_final_vy.mean()),\n",
        "                \"mean_final_abs_angle_success\": float(success_final_angle.mean()),\n",
        "            }\n",
        "        )\n",
        "    else:\n",
        "        # No success episodes: set smooth-landing metrics to None\n",
        "        metrics.update(\n",
        "            {\n",
        "                \"mean_final_abs_x_success\": None,\n",
        "                \"mean_final_abs_vy_success\": None,\n",
        "                \"mean_final_abs_angle_success\": None,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\")\n",
        "\n",
        "def random_policy(obs):\n",
        "    # `obs` is required by the interface but not used here.\n",
        "    return env.action_space.sample()\n",
        "\n",
        "metrics_random = evaluate_policy(env, random_policy, num_episodes=100, seed=0)\n",
        "print(\"Random policy metrics:\")\n",
        "for k, v in metrics_random.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_PHy7z4IDbZ",
        "outputId": "08e59b20-9bc3-43ac-e753-ac4ecb56de77"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random policy metrics:\n",
            "num_episodes: 100\n",
            "mean_return: -187.19863891601562\n",
            "std_return: 106.7403564453125\n",
            "min_return: -507.28753662109375\n",
            "max_return: 19.70433235168457\n",
            "solved_rate: 0.0\n",
            "success_rate: 0.0\n",
            "crash_rate: 1.0\n",
            "timeout_rate: 0.0\n",
            "other_terminal_rate: 0.0\n",
            "mean_episode_length: 89.9\n",
            "mean_main_engine_usage: 23.24\n",
            "mean_side_engine_usage: 44.22\n",
            "num_success_episodes: 0\n",
            "mean_final_abs_x_success: None\n",
            "mean_final_abs_vy_success: None\n",
            "mean_final_abs_angle_success: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.envs.box2d.lunar_lander import heuristic\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\")\n",
        "\n",
        "def heuristic_policy(obs):\n",
        "    # Use the official heuristic controller provided by Gymnasium.\n",
        "    return heuristic(env, obs)\n",
        "\n",
        "metrics_heuristic = evaluate_policy(env, heuristic_policy, num_episodes=100, seed=0)\n",
        "print(\"Heuristic policy metrics:\")\n",
        "for k, v in metrics_heuristic.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tvMlo2zIJwC",
        "outputId": "9da44c8d-a9e7-4df9-c873-35c06bb9e77c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heuristic policy metrics:\n",
            "num_episodes: 100\n",
            "mean_return: 240.22869873046875\n",
            "std_return: 97.566650390625\n",
            "min_return: -218.28501892089844\n",
            "max_return: 314.9457702636719\n",
            "solved_rate: 0.89\n",
            "success_rate: 0.93\n",
            "crash_rate: 0.05\n",
            "timeout_rate: 0.02\n",
            "other_terminal_rate: 0.0\n",
            "mean_episode_length: 252.98\n",
            "mean_main_engine_usage: 15.04\n",
            "mean_side_engine_usage: 107.14\n",
            "num_success_episodes: 93\n",
            "mean_final_abs_x_success: 0.11925307661294937\n",
            "mean_final_abs_vy_success: 0.0\n",
            "mean_final_abs_angle_success: 0.04345544055104256\n"
          ]
        }
      ]
    }
  ]
}